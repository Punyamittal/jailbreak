"""
Test Intent-Based Model on Jailbreak Dataset

Tests the trained intent-based detector on the provided jailbreak dataset.
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple
from collections import Counter
import pickle

# Metrics
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

def load_jailbreak_dataset(json_data: List[Dict]) -> Tuple[List[str], List[str]]:
    """
    Load prompts and labels from the jailbreak dataset.
    
    Args:
        json_data: List of dictionaries from the JSON dataset
        
    Returns:
        Tuple of (prompts, labels) where labels are 'jailbreak' or 'benign'
    """
    prompts = []
    labels = []
    
    for item in json_data:
        prompt = item.get('prompt', '').strip()
        if not prompt:
            continue
            
        # Ground truth: jailbroken=True means it's a jailbreak
        is_jailbreak = item.get('jailbroken', False)
        label = 'jailbreak' if is_jailbreak else 'benign'
        
        prompts.append(prompt)
        labels.append(label)
    
    return prompts, labels


def test_intent_model(detector, prompts: List[str], labels: List[str]) -> Dict:
    """
    Test the intent-based detector on the dataset.
    
    Returns:
        Dictionary with metrics and predictions
    """
    print("\n" + "="*70)
    print("TESTING INTENT MODEL ON JAILBREAK DATASET")
    print("="*70)
    
    print(f"\nTesting {len(prompts)} prompts...")
    
    predictions = []
    confidences = []
    decisions = []
    
    for i, prompt in enumerate(prompts):
        if (i + 1) % 50 == 0:
            print(f"  Processed {i + 1}/{len(prompts)}...")
        
        try:
            result = detector.detect(prompt)
            decision = result['decision']
            
            # Map decision to label
            if decision == 'block':
                predicted = 'jailbreak'
            elif decision == 'warn':
                predicted = 'jailbreak'  # Conservative: warn = treat as jailbreak
            else:
                predicted = 'benign'
            
            predictions.append(predicted)
            confidences.append(result['confidence'])
            decisions.append(decision)
            
        except Exception as e:
            print(f"  [ERROR] Failed on prompt {i+1}: {e}")
            predictions.append('benign')  # Default to benign on error
            confidences.append(0.0)
            decisions.append('allow')
    
    # Calculate metrics
    accuracy = accuracy_score(labels, predictions)
    precision = precision_score(labels, predictions, pos_label='jailbreak', zero_division=0)
    recall = recall_score(labels, predictions, pos_label='jailbreak', zero_division=0)
    f1 = f1_score(labels, predictions, pos_label='jailbreak', zero_division=0)
    
    # Confusion matrix
    cm = confusion_matrix(labels, predictions, labels=['benign', 'jailbreak'])
    
    # Per-class metrics
    tn, fp, fn, tp = cm.ravel()
    
    # False positive rate (benign classified as jailbreak)
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
    
    # False negative rate (jailbreak classified as benign) - CRITICAL for security
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0
    
    # Jailbreak recall (how many jailbreaks we caught)
    jailbreak_recall = recall
    
    # Benign precision (how many "benign" predictions were actually benign)
    benign_precision = tn / (tn + fn) if (tn + fn) > 0 else 0.0
    
    results = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'true_positives': int(tp),
        'false_negatives': int(fn),
        'false_positive_rate': fpr,
        'false_negative_rate': fnr,
        'jailbreak_recall': jailbreak_recall,
        'benign_precision': benign_precision,
        'predictions': predictions,
        'labels': labels,
        'confidences': confidences,
        'decisions': decisions
    }
    
    return results


def print_results(results: Dict):
    """Print comprehensive test results."""
    print("\n" + "="*70)
    print("TEST RESULTS")
    print("="*70)
    
    print(f"\nğŸ“Š Overall Metrics:")
    print(f"  Accuracy:        {results['accuracy']:.2%}")
    print(f"  Precision:       {results['precision']:.2%}")
    print(f"  Recall:          {results['recall']:.2%}")
    print(f"  F1-Score:        {results['f1']:.2%}")
    
    print(f"\nğŸ¯ Security-Critical Metrics:")
    print(f"  Jailbreak Recall: {results['jailbreak_recall']:.2%} (must be >80%)")
    print(f"  False Negatives:  {results['false_negatives']} (jailbreaks missed)")
    print(f"  False Negative Rate: {results['false_negative_rate']:.2%} (must be <20%)")
    
    print(f"\nâš ï¸  Error Rates:")
    print(f"  False Positives:  {results['false_positives']} (benign flagged as jailbreak)")
    print(f"  False Positive Rate: {results['false_positive_rate']:.2%}")
    
    print(f"\nğŸ“ˆ Confusion Matrix:")
    print(f"                    Predicted")
    print(f"                  Benign  Jailbreak")
    print(f"  Actual Benign     {results['true_negatives']:5d}  {results['false_positives']:5d}")
    print(f"  Actual Jailbreak  {results['false_negatives']:5d}  {results['true_positives']:5d}")
    
    # Per-class accuracy
    total_benign = results['true_negatives'] + results['false_positives']
    total_jailbreak = results['false_negatives'] + results['true_positives']
    
    benign_accuracy = results['true_negatives'] / total_benign if total_benign > 0 else 0.0
    jailbreak_accuracy = results['true_positives'] / total_jailbreak if total_jailbreak > 0 else 0.0
    
    print(f"\nğŸ“‹ Per-Class Accuracy:")
    print(f"  Benign:     {benign_accuracy:.2%} ({results['true_negatives']}/{total_benign})")
    print(f"  Jailbreak:  {jailbreak_accuracy:.2%} ({results['true_positives']}/{total_jailbreak})")
    
    # Show some examples
    print(f"\nğŸ” Sample Predictions:")
    
    # False negatives (missed jailbreaks) - CRITICAL
    fn_indices = [i for i, (pred, label) in enumerate(zip(results['predictions'], results['labels'])) 
                  if pred == 'benign' and label == 'jailbreak']
    
    if fn_indices:
        print(f"\n  âŒ False Negatives (Missed Jailbreaks) - {len(fn_indices)} examples:")
        for idx in fn_indices[:5]:  # Show first 5
            prompt = results.get('prompts', [''])[idx] if 'prompts' in results else f"Prompt {idx+1}"
            conf = results['confidences'][idx]
            print(f"    [{idx+1}] Confidence: {conf:.2%}")
            print(f"        Prompt: {prompt[:100]}...")
    
    # False positives (benign flagged as jailbreak)
    fp_indices = [i for i, (pred, label) in enumerate(zip(results['predictions'], results['labels'])) 
                  if pred == 'jailbreak' and label == 'benign']
    
    if fp_indices:
        print(f"\n  âš ï¸  False Positives (Benign Flagged) - {len(fp_indices)} examples:")
        for idx in fp_indices[:5]:  # Show first 5
            prompt = results.get('prompts', [''])[idx] if 'prompts' in results else f"Prompt {idx+1}"
            conf = results['confidences'][idx]
            print(f"    [{idx+1}] Confidence: {conf:.2%}")
            print(f"        Prompt: {prompt[:100]}...")
    
    # High confidence correct predictions
    correct_high_conf = []
    for i, (pred, label) in enumerate(zip(results['predictions'], results['labels'])):
        if pred == label and results['confidences'][i] > 0.8:
            correct_high_conf.append(i)
    
    if correct_high_conf:
        print(f"\n  âœ… High Confidence Correct - {len(correct_high_conf)} examples:")
        for idx in correct_high_conf[:3]:  # Show first 3
            prompt = results.get('prompts', [''])[idx] if 'prompts' in results else f"Prompt {idx+1}"
            conf = results['confidences'][idx]
            label = results['labels'][idx]
            print(f"    [{idx+1}] {label.upper()} - Confidence: {conf:.2%}")
            print(f"        Prompt: {prompt[:100]}...")
    
    print("\n" + "="*70)


def main():
    """Main test function."""
    print("="*70)
    print("TESTING INTENT MODEL ON JAILBREAK DATASET")
    print("="*70)
    
    # Load the dataset from the provided JSON
    # The user provided the data as a JSON array in the query
    # We'll need to parse it
    
    # For now, we'll expect the data to be passed or loaded from a file
    # Let's create a way to load it from the query or a file
    
    # Check if dataset file exists
    dataset_file = Path("jailbreak_test_dataset.json")
    
    if not dataset_file.exists():
        print("\n[ERROR] Dataset file not found!")
        print("Please save the JSON data to 'jailbreak_test_dataset.json'")
        print("\nOr paste the JSON data when running this script.")
        return
    
    # Load dataset
    print(f"\n[1/3] Loading dataset from {dataset_file}...")
    with open(dataset_file, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    prompts, labels = load_jailbreak_dataset(json_data)
    print(f"  Loaded {len(prompts)} examples")
    
    label_counts = Counter(labels)
    print(f"\n  Label distribution:")
    for label, count in label_counts.items():
        print(f"    {label}: {count} ({count/len(labels)*100:.1f}%)")
    
    # Load model
    print(f"\n[2/3] Loading intent model...")
    model_path = Path("models/intent_detector.pkl")
    
    if not model_path.exists():
        print(f"[ERROR] Model not found at {model_path}")
        print("Please train the model first: python train_intent_model.py")
        return
    
    try:
        with open(model_path, 'rb') as f:
            detector = pickle.load(f)
        print(f"  [OK] Model loaded from {model_path}")
    except Exception as e:
        print(f"  [ERROR] Failed to load model: {e}")
        return
    
    # Test
    print(f"\n[3/3] Testing model...")
    results = test_intent_model(detector, prompts, labels)
    
    # Add prompts to results for display
    results['prompts'] = prompts
    
    # Print results
    print_results(results)
    
    # Save results
    results_file = Path("test_results_jailbreak_dataset.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        # Convert numpy arrays to lists for JSON
        results_json = {
            'accuracy': float(results['accuracy']),
            'precision': float(results['precision']),
            'recall': float(results['recall']),
            'f1': float(results['f1']),
            'jailbreak_recall': float(results['jailbreak_recall']),
            'false_negative_rate': float(results['false_negative_rate']),
            'false_positive_rate': float(results['false_positive_rate']),
            'confusion_matrix': results['confusion_matrix'].tolist(),
            'true_negatives': int(results['true_negatives']),
            'false_positives': int(results['false_positives']),
            'true_positives': int(results['true_positives']),
            'false_negatives': int(results['false_negatives']),
        }
        json.dump(results_json, f, indent=2)
    
    print(f"\n[OK] Results saved to {results_file}")
    
    # Final verdict
    print("\n" + "="*70)
    if results['jailbreak_recall'] >= 0.80 and results['false_negative_rate'] < 0.20:
        print("âœ… MODEL MEETS SECURITY REQUIREMENTS")
    else:
        print("âš ï¸  MODEL NEEDS IMPROVEMENT")
        if results['jailbreak_recall'] < 0.80:
            print(f"   - Jailbreak recall ({results['jailbreak_recall']:.2%}) below target (80%)")
        if results['false_negative_rate'] >= 0.20:
            print(f"   - False negative rate ({results['false_negative_rate']:.2%}) above target (20%)")
    print("="*70)


if __name__ == "__main__":
    main()

